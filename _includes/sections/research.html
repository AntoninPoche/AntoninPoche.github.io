<div class="card">
  <p>The goal of my PhD is to build an interpretability agent (conversational explainability) for language models.
    However, I find that the quality and consistency of explanations are lacking, in particular with concept-based
    explanations.
  </p>
  <p>Which is why I currently work on improving such explanations.
    Notably, the first publication of my PhD,
    <a class="link" href="https://arxiv.org/abs/2501.05855">ConSim (Poch√© et al. ACL 2025)</a>,
    aims at evaluating the usefulness of concept-based explanations in NLP.
  </p>
  <p>The future directions I would like to explore are:</p>
  <ul>
    <li>Improving the interpretation of concept-based explanations.</li>
    <li>Building contrastive concept-based explanations.</li>
    <li>Example-based explanations for language models.</li>
  </ul>
  <p>If these subjects are of interest to you, feel free to contact me, I would be happy to collaborate.</p>
</div>